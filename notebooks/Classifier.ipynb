{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "In this notebook we will try out different classifiers. We will be using different versions of feature analysis to show which one is the best. Lastly we will compare the scores and the classifier with the highest scores will be used for the widget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SimpleCV import *\n",
    "import numpy as np\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation\n",
    "We are using cross validation to lower the change of overfitting. We are using it in two ways. \n",
    "\n",
    "The datasets that the classifiers use has been split up. This is done below. We make a training set which has 80% of the data in it. We also make a test set with 20% of the data in it. A classifier will train with the training set, and to test if it trained wel the test set can be used. \n",
    "\n",
    "Another way that we are using cross validation is with k-fold. This splits your data randomly into an x amount of folds. One fold will serve as a validation set and the others as a training set. This method is used in the parameter tuning. GridSearch uses this by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset-numpy/dataset_analysis_normalized_v3.csv\")\n",
    "df.head()\n",
    "\n",
    "# split df in data (X) and labels (y)\n",
    "X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "# create train and test data and labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.20, random_state=67)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "PCA stands for pricipal components analysis. With PCA the dimensions of data is reduced, it is summarized. In our case we took 0.95 procent of the components. When testing our classifiers with the PSA dataset some classifiers decreased in score and some increased. We made the choice to only use the PCA datasets on the classifiers where the score increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.95)\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "We chose two naive bays models. Gaussian which can be used if a dataset has a normal distribution. Bernoulli which works better if the features are with zeros and ones. The outputs of these classifiers where rather low it would not reach the level of other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian : 0.94270833333333337\n",
    "Gaussian worked better with the PCA dataset. For this classifier the most confusion was around the number 9. The number was seen for many other numbers like 1,2,7 and 8. The only numbers that were not confused are 0 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Gaussian\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_pca, y_train)\n",
    "#print \"Gaussian :\",gnb.score(X_test_pca, y_test)\n",
    "prediction=gnb.predict(X_test_pca)\n",
    "accuracy_score_gnb = metrics.accuracy_score(prediction,y_test)\n",
    "\n",
    "#evaluation(Accuracy)\n",
    "print \"Accuracy:\",accuracy_score_gnb \n",
    "#evaluation(Confusion Metrix)\n",
    "print metrics.confusion_matrix(prediction,y_test)\n",
    "print metrics.classification_report(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bernoulli: 0.84895833333333337\n",
    "We also tried to use the Bernoulli classifier. This classifier scored rather low. This is because our dataset does not only consist out of zeros and ones but also out of other numbers. When looking at the confusion matrix every number has been confused for other numbers except 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Bernoulli\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "#print \"Bernoulli:\",bnb.score(X_test, y_test)\n",
    "prediction=bnb.predict(X_test)\n",
    "accuracy_score_bnb = metrics.accuracy_score(prediction,y_test)\n",
    "#evaluation(Accuracy)\n",
    "print \"Accuracy:\",accuracy_score_bnb\n",
    "#evaluation(Confusion Metrix)\n",
    "print metrics.confusion_matrix(prediction,y_test)\n",
    "print metrics.classification_report(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfRF = RandomForestClassifier()\n",
    "clfRF.fit(X_train, y_train)\n",
    "prediction=clfRF.predict(X_test)\n",
    "\n",
    "accuracy_score_rf = metrics.accuracy_score(prediction,y_test)\n",
    "#evaluation(Accuracy)\n",
    "print \"Accuracy:\",accuracy_score_rf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomize search\n",
    "When running random forest with default settings the score was about 0.945. The descision was made to tune the hyper parameters. To get the best possible parameters we used the randomized search in combination with the grid search. This tries all the combinations between a certain range. When using the randomized search we used k-fold cross validation, this helps with overfitting. It this case the k-fold is 5 (cv=5). This means it will randomly split the dataset into 5 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of trees \n",
    "n_estimators=[int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)] \n",
    "\n",
    "#number of features considered for splitting at leaf node\n",
    "max_features =['sqrt', 'auto']\n",
    "\n",
    "#method for sampling data points (with or without replacement)\n",
    "bootstrap=[True, False]\n",
    "\n",
    "#min number of data points allowed in a leaf node\n",
    "min_samples_leaf=[1,2,4]\n",
    "\n",
    "#min number of data points placed in a node before the node is split\n",
    "min_samples_split=[2,5,10]\n",
    "\n",
    "#max number of levels in each decision tree\n",
    "max_depth=[int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "randomgrid = {'n_estimators':n_estimators,\n",
    "             'max_features': max_features,\n",
    "             'bootstrap': bootstrap,\n",
    "             'min_samples_leaf': min_samples_leaf,\n",
    "             'min_samples_split': min_samples_split,\n",
    "             'max_depth': max_depth}\n",
    "\n",
    "clfRF = RandomForestClassifier()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = clfRF, param_distributions = randomgrid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The best params that came out of the randomize search:\n",
    "  bootstrap: False\n",
    "  <br>\n",
    "  max_depth: 20\n",
    "  <br>\n",
    "  max_features: 'sqrt'\n",
    "  <br>\n",
    "  min_samples_leaf: 1\n",
    "  <br>\n",
    "  min_samples_split: 2\n",
    "  <br>\n",
    "  n_estimators: 1800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search\n",
    "To get even better hyper parameters, we are using the result of the randomize search to narrow down the range for the grid search. Grid search also uses k fold cross validation, in this case it is set to 3 folds. There is a change that this score could be better, but it could also descrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "parameters = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [10,20,30,40,50],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [1],\n",
    "    'min_samples_split': [2],\n",
    "    'n_estimators': [1400,1500,1600,1800,1900,2000]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "clfRF = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = clfRF, param_grid = parameters, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The best params that came out of the grid search\n",
    "bootstrap: False\n",
    "<br>\n",
    "max_depth: 40\n",
    "<br>\n",
    "max_features: 'sqrt'\n",
    "<br>\n",
    "min_samples_leaf: 1\n",
    "<br>\n",
    "min_samples_split: 2\n",
    "<br>\n",
    "n_estimators: 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the default setting of the random forest the acurracy score was about 0.945. Running the random forest with the tuned parameters the accurracy score came to 0.9765625. To be sure we also ran the params that came out of the randomize search. This was an accurracy score of 0.979166666667, which is higher than the grid search. \n",
    "This is an increase of about 3%. \n",
    "\n",
    "When looking at the confusion matrix number 8 catches our attention. Number 8 was mismatched to a 1 and 6. Also number 9 gets confused with 1 and 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest with parameters from random and grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfRF = RandomForestClassifier(bootstrap= False,  max_depth= 20, max_features= 'sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 1800)\n",
    "clfRF.fit(X_train, y_train)\n",
    "prediction=clfRF.predict(X_test)\n",
    "\n",
    "accuracy_score_rf = metrics.accuracy_score(prediction,y_test)\n",
    "#evaluation(Accuracy)\n",
    "print \"Accuracy :\",accuracy_score_rf \n",
    "#evaluation(Confusion Metrix)\n",
    "print metrics.confusion_matrix(prediction,y_test) \n",
    "print metrics.classification_report(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree\n",
    "The decision tree scored with default settings only  0.86979166666666663. Tuning parameters will not get this classifier to the accurracy scores of other classifiers. When looking at the confusion matrix 6 is the only number that does not get confused. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfDT = DecisionTreeClassifier()\n",
    "clfDT.fit(X_train, y_train) \n",
    "prediction=clfDT.predict(X_test)\n",
    "\n",
    "accuracy_score_dt = metrics.accuracy_score(prediction,y_test)\n",
    "#evaluation(Accuracy)\n",
    "print(\"Accuracy:\",accuracy_score_dt)\n",
    "#evaluation(Confusion Metrix)\n",
    "print(metrics.confusion_matrix(prediction,y_test))\n",
    "print metrics.classification_report(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfKNN = KNeighborsClassifier()\n",
    "# Train the classifier with the training data and training labels\n",
    "clfKNN.fit(X_train, y_train)\n",
    "    # Score the classifier\n",
    "    # (calculates the mean accuracy of the given test data) \n",
    "prediction=clfKNN.predict(X_test)\n",
    "print \"Accuracy: \", metrics.accuracy_score(prediction,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the KNN we first let it run with default settings. The accurracy score was already near the 0.97135416666666663 . So we decided to try to make this higher with tuning the hyper parameters. We first ran the classifier with an iteration. Where it would try the neighbors 1-9 in the params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (1,10):\n",
    "    # Create a classifier with k \n",
    "    clfKNN = KNeighborsClassifier(n_neighbors=i)\n",
    "    # Train the classifier with the training data and training labels\n",
    "    clfKNN.fit(X_train, y_train)\n",
    "    # Score the classifier\n",
    "    # (calculates the mean accuracy of the given test data) \n",
    "    prediction=clfKNN.predict(X_test)\n",
    "    print \"Accuracy: \", i ,metrics.accuracy_score(prediction,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN with neighbor iteration\n",
    "Accuracy: 0.96354166666666663 Neighbors: 1\n",
    "<br>\n",
    "Accuracy: 0.96354166666666663 Neighbors: 2\n",
    "<br>\n",
    "Accuracy: 0.97135416666666663 Neighbors: 3\n",
    "<br>\n",
    "Accuracy: 0.96875........................ Neighbors: 4\n",
    "<br>\n",
    "Accuracy: 0.97135416666666663 Neighbors: 5\n",
    "<br>\n",
    "Accuracy: 0.96614583333333337 Neighbors: 6\n",
    "<br>\n",
    "Accuracy: 0.96354166666666663 Neighbors: 7\n",
    "<br>\n",
    "Accuracy: 0.96614583333333337 Neighbors: 8\n",
    "<br>\n",
    "Accuracy: 0.96614583333333337 Neighbors: 9\n",
    "<br>\n",
    "<br>\n",
    "The highest accurracy score of running the classifier with an iterator is the same as when running it default 0.97135416666666663. As the iteration only focuced on the neighbors we decided to use grid search. We now tuned more than one parameter.\n",
    "<br>\n",
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the instance\n",
    "clfKNN = KNeighborsClassifier(n_jobs=-1)\n",
    "#Hyper Parameters Set\n",
    "params = {'n_neighbors':[1,2,3,4,5,6,7,8,9,10],\n",
    "          'leaf_size':[1,2,3,5],\n",
    "          'weights':['uniform', 'distance'],\n",
    "          'algorithm':['auto', 'ball_tree','kd_tree','brute'],\n",
    "          'n_jobs':[-1]}\n",
    "#Making models with hyper parameters sets\n",
    "clfKNN1 = GridSearchCV(clfKNN, param_grid=params, n_jobs=-1, verbose=1)\n",
    "#Learning\n",
    "clfKNN1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best params for KNN with grid search\n",
    "algorithm': 'auto'\n",
    "<br>\n",
    "leaf_size': 1\n",
    "<br>\n",
    "n_jobs': -1\n",
    "<br>\n",
    "n_neighbors': 6\n",
    "<br>\n",
    "weights': 'distance'\n",
    "<br>\n",
    "<br>\n",
    "Running the classifier with the params from the gridsearch improved the accurracy score. It is now 0.97395833333333337. It is an improvement of about 0.20%. When looking at the confusion matrix numbers 4 and 9 stand out. Number 4 gets confused with 1 and 8. Number 9 get confused with 1,4 and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.973958333333\n",
      "[[42  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 38  1  1  1  0  0  0  1  1]\n",
      " [ 0  0 33  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 39  0  1  0  0  0  0]\n",
      " [ 0  1  0  0 27  0  0  0  0  1]\n",
      " [ 0  0  0  0  0 28  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 32  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 51  0  1]\n",
      " [ 0  0  0  0  1  0  0  0 34  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 50]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        42\n",
      "          1       0.97      0.88      0.93        43\n",
      "          2       0.97      1.00      0.99        33\n",
      "          3       0.97      0.97      0.97        40\n",
      "          4       0.93      0.93      0.93        29\n",
      "          5       0.97      1.00      0.98        28\n",
      "          6       1.00      1.00      1.00        32\n",
      "          7       1.00      0.98      0.99        52\n",
      "          8       0.97      0.97      0.97        35\n",
      "          9       0.94      1.00      0.97        50\n",
      "\n",
      "avg / total       0.97      0.97      0.97       384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfKNN = KNeighborsClassifier(n_neighbors = 6, n_jobs = -1, weights= 'distance', leaf_size= 1, algorithm= 'auto')\n",
    "clfKNN.fit(X_train, y_train)\n",
    "prediction=clfKNN.predict(X_test)\n",
    "\n",
    "accuracy_score_knn = metrics.accuracy_score(prediction,y_test)\n",
    "#evaluation(Accuracy)\n",
    "print \"Accuracy:\",accuracy_score_knn \n",
    "#evaluation(Confusion Metrix)\n",
    "print metrics.confusion_matrix(prediction,y_test)\n",
    "print metrics.classification_report(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD\n",
    "The accurracy score when running SGD with default settings is 0.9375. With changing the params manually we came to a score of 0.958333333333. When looking at the confusion matrix number 1 and 8 stand out. Number 1 gets confused for a 2 and 4. Number 8 gets confused for number 1 and number 5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.958333333333\n",
      "[[42  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 35  1  0  1  0  0  0  3  2]\n",
      " [ 0  2 33  0  0  1  0  1  0  0]\n",
      " [ 0  0  0 40  0  0  0  0  0  0]\n",
      " [ 0  2  0  0 28  0  0  0  0  2]\n",
      " [ 0  0  0  0  0 28  0  0  1  0]\n",
      " [ 0  0  0  0  0  0 32  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 50  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 31  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 49]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        42\n",
      "          1       0.90      0.83      0.86        42\n",
      "          2       0.97      0.89      0.93        37\n",
      "          3       1.00      1.00      1.00        40\n",
      "          4       0.97      0.88      0.92        32\n",
      "          5       0.97      0.97      0.97        29\n",
      "          6       1.00      1.00      1.00        32\n",
      "          7       0.98      1.00      0.99        50\n",
      "          8       0.89      1.00      0.94        31\n",
      "          9       0.92      1.00      0.96        49\n",
      "\n",
      "avg / total       0.96      0.96      0.96       384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfSGD = linear_model.SGDClassifier(loss = 'log', n_iter =1000)\n",
    "clfSGD.fit(X_train, y_train)\n",
    "prediction=clfSGD.predict(X_test)\n",
    "\n",
    "accuracy_score_sgd = metrics.accuracy_score(prediction,y_test)\n",
    "#evaluation(Accuracy)\n",
    "print \"Accuracy:\",accuracy_score_sgd \n",
    "#evaluation(Confusion Metrix)\n",
    "print metrics.confusion_matrix(prediction,y_test) \n",
    "print metrics.classification_report(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default SVM with Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with Nu-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVC\n",
    "clf = NuSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with Linear Support Vector Classifiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC()m\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these three types of support vector machines, the __SVC__ scores the highest. We will use this one and tweak the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with Gridsearch\n",
    "\n",
    "Using grid search we can find which parameters are the best to get the highest score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel' : ('linear', 'rbf'), 'C': np.arange(1, 2, 0.1), \n",
    "              'gamma': np.arange(.1, .5, 0.05)}\n",
    "\n",
    "clf = GridSearchCV(svm.SVC(), parameters, verbose=1, n_jobs=-1, cv = 5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_test, y_test)\n",
    "\n",
    "print clf.best_params_\n",
    "print clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros((100))\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.20)\n",
    "    clf = svm.SVC(kernel =\"rbf\", C=1.2, gamma=0.3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores[i] = clf.score(X_test, y_test)\n",
    "\n",
    "print 'mean', scores.mean()\n",
    "\n",
    "print 'min', scores.min()\n",
    "print 'max', scores.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best params for SVM with grid search\n",
    "kernel': 'rbf'\n",
    "<br>\n",
    "C': 1.2\n",
    "<br>\n",
    "gamma': 0.171\n",
    "\n",
    "This scores 0.979 on average when we run it with these parameters a 100 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros((100))\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.20)\n",
    "    pca = PCA(0.95)\n",
    "    pca.fit(X_train)\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    clf = svm.SVC(kernel =\"rbf\", C=1.2, gamma=0.3)\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "    scores[i] = clf.score(X_test_pca, y_test)\n",
    "\n",
    "print 'mean', scores.mean()\n",
    "\n",
    "print 'min', scores.min()\n",
    "print 'max', scores.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scores 0.979 on average when we run it with these parameters a 100 times. It scores the same as without PCA, so we decided to leave it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Classifier SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel =\"rbf\", C=1.2, gamma=0.3)\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "labels = y_test\n",
    "\n",
    "print confusion_matrix (labels, predictions)\n",
    "print classification_report(labels, predictions)\n",
    "print 'accuracy: ', metrics.accuracy_score(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested svm on different datasets to see which would score the best\n",
    "\n",
    "- __dataset_analysis_normalized_v3.csv - the latest normalized version__ <br>\n",
    "0.979505208333\n",
    "- dataset_analysis_v3.csv - the latest version not normalized <br>\n",
    "0.090546875\n",
    "- dataset_analysis_normalized_with_contours.csv - normalized version without the column 'blob_amount_contours'  <br>\n",
    "0.979479166667\n",
    "- dataset_analysis_with_contours.csv - not normalized version without the column 'blob_amount_contours'  <br>\n",
    "0.0909635416667\n",
    "\n",
    "The not normalized versions scored the worst. These are not useful at all. The version without 'blob_amount_contours' lowers the score a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "labels = y_test\n",
    "\n",
    "print confusion_matrix (labels, predictions)\n",
    "print classification_report(labels, predictions)\n",
    "print metrics.accuracy_score(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "When running LG with default settings a score around 0.9583 comes out. We decided to tune the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.958333333333\n",
      "[[42  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 36  1  0  1  0  0  0  3  1]\n",
      " [ 0  2 33  0  0  1  0  1  0  0]\n",
      " [ 0  0  0 40  0  0  0  0  0  0]\n",
      " [ 0  1  0  0 27  0  0  0  0  2]\n",
      " [ 0  0  0  0  0 27  0  0  1  0]\n",
      " [ 0  0  0  0  0  0 32  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 50  0  0]\n",
      " [ 0  0  0  0  1  0  0  0 31  0]\n",
      " [ 0  0  0  0  0  1  0  0  0 50]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        42\n",
      "          1       0.92      0.86      0.89        42\n",
      "          2       0.97      0.89      0.93        37\n",
      "          3       1.00      1.00      1.00        40\n",
      "          4       0.93      0.90      0.92        30\n",
      "          5       0.93      0.96      0.95        28\n",
      "          6       1.00      1.00      1.00        32\n",
      "          7       0.98      1.00      0.99        50\n",
      "          8       0.89      0.97      0.93        32\n",
      "          9       0.94      0.98      0.96        51\n",
      "\n",
      "avg / total       0.96      0.96      0.96       384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfLG = LogisticRegression()\n",
    "clfLG.fit(X_train, y_train)\n",
    "prediction=clfLG.predict(X_test)\n",
    "accuracy_score_lg = metrics.accuracy_score(prediction,y_test)\n",
    "#evaluation(Accuracy)\n",
    "print \"Accuracy:\",accuracy_score_lg \n",
    "#evaluation(Confusion Metrix)\n",
    "print metrics.confusion_matrix(prediction,y_test)\n",
    "print metrics.classification_report(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('logreg',LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\"))])\n",
    "parameters = {'logreg__C':np.arange(0.01,100,10)}\n",
    "clfLG = GridSearchCV(pipeline,parameters,cv=5, n_jobs=-1, verbose =2)\n",
    "clfLG.fit(X_train, y_train)\n",
    "print(\"Best parameters:\\n\",clfLG.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Best params gridsearch\n",
    "logreg__C: 10.01\n",
    "<br>\n",
    "<br>\n",
    "When testing the LG classifier with this param the results went down to about 0.9557. So the best option for LG is to use the default settings. When looking at the matrix of the default settings number 1 stands out. It is confused for a 2, 4, 8 and 9. Number 4 stands out as well and is confused with a 1 and 9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Best params gridsearch\n",
    "logreg__C: 10.01\n",
    "<br>\n",
    "<br>\n",
    "When testing the LG classifier with this param the results went down to about 0.9557. So the best option for LG is to use the default settings. When looking at the matrix of the default settings number 1 stands out. It is confused for a 2, 4, 8 and 9. Number 4 stands out as well and is confused with a 1 and 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clfLG = LogisticRegression(random_state=0, solver='lbfgs',  multi_class='multinomial', C =10.01 )\n",
    "clfLG.fit(X_train, y_train)\n",
    "prediction=clfLG.predict(X_test)\n",
    "#evaluation(Accuracy)\n",
    "print \"Accuracy:\",metrics.accuracy_score(prediction,y_test)\n",
    "#evaluation(Confusion Metrix)\n",
    "print metrics.confusion_matrix(prediction,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_svm = scores.mean()\n",
    "dfResults = pd.Series([accuracy_score_gnb*100, accuracy_score_bnb*100,accuracy_score_rf*100, accuracy_score_dt*100,\n",
    "                       accuracy_score_knn*100, accuracy_score_sgd*100, accuracy_score_svm*100, accuracy_score_lg*100],\n",
    "                      index=['Gaussian', 'Bernoulli', 'Random Forest', 'Decision Tree',\n",
    "                             'KNN', 'SGD','SVM', 'Logistic Regression'])\n",
    "dfResults.head(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
