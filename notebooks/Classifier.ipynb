{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "In this notebook we will try out different classifiers. We will be using different versions of feature analysis to show which one is the best. Lastly we will compare the scores and the classifier with the highest scores will be used for the widget. The best classifier turned out to be the SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SimpleCV import *\n",
    "import numpy as np\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers by order of score\n",
    "\n",
    "All the classifiers we have implemented with the corresponding accuracy score are shown below. We show the highest score per classifier. The SVM with tweaked parameters scores the best. We will use this one for the test application.\n",
    " \n",
    "GaussianNB: 0.942708333333 <br>\n",
    "Bernoulli: 0.869791666667 <br>\n",
    "Random Forest: 0.9765625 <br>\n",
    "Desicion Tree: 0.877604166667 <br>\n",
    "KNN: 0.966145833333 <br>\n",
    "SGD: 0.958333333333 <br>\n",
    "Logistic Regression: 0.9583 <br>\n",
    "__SVM: 0.980729166667__ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation\n",
    "We are using cross validation to lower the change of overfitting. We are using it in two ways. \n",
    "\n",
    "The datasets that the classifiers use has been split up. This is done below. We make a training set which has 80% of the data in it. We also make a test set with 20% of the data in it. A classifier will train with the training set, and to test if it trained wel the test set can be used. \n",
    "\n",
    "Another way that we are using cross validation is with k-fold. This splits your data randomly into an x amount of folds. One fold will serve as a validation set and the others as a training set. This method is used in the parameter tuning. GridSearch uses this by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset-numpy/dataset_analysis_normalized_v4.csv\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "# split df in data (X) and labels (y)\n",
    "X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "# create train and test data and labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.20, random_state=67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "PCA stands for pricipal components analysis. With PCA the dimensions of data is reduced, it is summarized. In our case we took 0.95 procent of the components. When testing our classifiers with the PSA dataset some classifiers decreased in score and some increased. We made the choice to only use the PCA datasets on the classifiers where the score increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.95)\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "We chose two naive bays models. Gaussian which can be used if a dataset has a normal distribution. Bernoulli which works better if the features are with zeros and ones. The outputs of these classifiers where rather low it would not reach the level of other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian : 0.942708333333\n",
    "Gaussian worked better with the PCA dataset. For this classifier the most confusion was around the number 9. The number was seen for many other numbers like 1,3 and 8. The only numbers that were not confused are 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.942708333333\n",
      "[[42  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 39  0  0  0  0  1  1  2  4]\n",
      " [ 0  0 33  1  0  0  0  0  0  0]\n",
      " [ 0  0  0 39  0  1  0  0  0  1]\n",
      " [ 0  0  0  0 28  0  1  2  0  0]\n",
      " [ 0  0  0  0  0 24  0  0  1  0]\n",
      " [ 0  0  0  0  0  0 30  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 48  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 32  1]\n",
      " [ 0  0  1  0  1  4  0  0  0 47]]\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Gaussian\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_pca, y_train)\n",
    "\n",
    "prediction=gnb.predict(X_test_pca)\n",
    "accuracy_score_gnb = metrics.accuracy_score(prediction,y_test)\n",
    "\n",
    "#evaluation\n",
    "print \"Accuracy:\",accuracy_score_gnb \n",
    "\n",
    "print metrics.confusion_matrix(prediction,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bernoulli: 0.869791666667\n",
    "We also tried to use the Bernoulli classifier. This classifier scored rather low. This is because our dataset does not only consist out of booleans but out of multiple numbers. When looking at the confusion matrix every number has been confused for other numbers except 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.869791666667\n",
      "[[42  0  0  0  0  1  0  0  0  0]\n",
      " [ 0 30  0  1  6  0  1  0  0  2]\n",
      " [ 0  4 30  2  0  0  0  0  0  0]\n",
      " [ 0  0  0 32  0  0  0  0  1  1]\n",
      " [ 0  1  0  0 22  0  0  2  0  4]\n",
      " [ 0  1  1  2  0 23  0  0  1  0]\n",
      " [ 0  0  1  0  0  0 31  0  0  0]\n",
      " [ 0  0  0  0  1  0  0 49  1  1]\n",
      " [ 0  3  1  1  0  1  0  0 32  2]\n",
      " [ 0  0  1  2  0  4  0  0  0 43]]\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Bernoulli\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "prediction=bnb.predict(X_test)\n",
    "accuracy_score_bnb = metrics.accuracy_score(prediction,y_test)\n",
    "\n",
    "#evaluation\n",
    "print \"Accuracy:\",accuracy_score_bnb\n",
    "print metrics.confusion_matrix(prediction,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest with default parameters: 0.950520833333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.950520833333\n"
     ]
    }
   ],
   "source": [
    "# default parameters\n",
    "clfRF = RandomForestClassifier()\n",
    "clfRF.fit(X_train, y_train)\n",
    "prediction=clfRF.predict(X_test)\n",
    "\n",
    "accuracy_score_rf = metrics.accuracy_score(prediction,y_test)\n",
    "\n",
    "#evaluation\n",
    "print \"Accuracy:\",accuracy_score_rf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomize search\n",
    "When running random forest with default settings the score was about 0.95. The descision was made to tune the hyper parameters. To get the best possible parameters we used the randomized search in combination with the grid search. This tries all the combinations between a certain range. When using the randomized search we used k-fold cross validation, this helps with overfitting. It this case the k-fold is 5 (cv=5). This means it will randomly split the dataset into 5 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of trees \n",
    "n_estimators=[int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)] \n",
    "\n",
    "#number of features considered for splitting at leaf node\n",
    "max_features =['sqrt', 'auto']\n",
    "\n",
    "#method for sampling data points (with or without replacement)\n",
    "bootstrap=[True, False]\n",
    "\n",
    "#min number of data points allowed in a leaf node\n",
    "min_samples_leaf=[1,2,4]\n",
    "\n",
    "#min number of data points placed in a node before the node is split\n",
    "min_samples_split=[2,5,10]\n",
    "\n",
    "#max number of levels in each decision tree\n",
    "max_depth=[int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "randomgrid = {'n_estimators':n_estimators,\n",
    "             'max_features': max_features,\n",
    "             'bootstrap': bootstrap,\n",
    "             'min_samples_leaf': min_samples_leaf,\n",
    "             'min_samples_split': min_samples_split,\n",
    "             'max_depth': max_depth}\n",
    "\n",
    "clfRF = RandomForestClassifier()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = clfRF, param_distributions = randomgrid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The best params that came out of the randomize search:\n",
    "  bootstrap: False\n",
    "  <br>\n",
    "  max_depth: 20\n",
    "  <br>\n",
    "  max_features: 'sqrt'\n",
    "  <br>\n",
    "  min_samples_leaf: 1\n",
    "  <br>\n",
    "  min_samples_split: 2\n",
    "  <br>\n",
    "  n_estimators: 1800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search\n",
    "To get even better hyper parameters, we are using the result of the randomize search to narrow down the range for the grid search. Grid search also uses k fold cross validation, in this case it is set to 3 folds. There is a chance that this score could be better, but it could also descrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "parameters = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [10,20,30,40,50],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [1],\n",
    "    'min_samples_split': [2],\n",
    "    'n_estimators': [1400,1500,1600,1800,1900,2000]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "clfRF = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = clfRF, param_grid = parameters, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The best params that came out of the grid search\n",
    "bootstrap: False\n",
    "<br>\n",
    "max_depth: 40\n",
    "<br>\n",
    "max_features: 'sqrt'\n",
    "<br>\n",
    "min_samples_leaf: 1\n",
    "<br>\n",
    "min_samples_split: 2\n",
    "<br>\n",
    "n_estimators: 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest with parameters from random and grid search:  0.9765625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the default setting of the random forest the acurracy score was about 0.95. The gridSearch and randomized search both gave the same parameters. These parameters increased the score with almost 3%.\n",
    "\n",
    "When looking at the confusion matrix number 5 catches our attention. Number 5 was mismatched to a 2 and a 9. Also number 9 gets confused with 4 and 8. The 0, 2, 3, 4, 7 and 8 are alle predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9765625\n",
      "[[42  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 38  0  0  0  0  1  0  0  0]\n",
      " [ 0  0 34  0  0  2  0  0  0  0]\n",
      " [ 0  1  0 40  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 29  0  0  0  0  2]\n",
      " [ 0  0  0  0  0 26  1  0  0  0]\n",
      " [ 0  0  0  0  0  0 30  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 51  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 35  1]\n",
      " [ 0  0  0  0  0  1  0  0  0 50]]\n"
     ]
    }
   ],
   "source": [
    "clfRF = RandomForestClassifier(bootstrap= False,  max_depth= 20, max_features= 'sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 1800)\n",
    "clfRF.fit(X_train, y_train)\n",
    "prediction=clfRF.predict(X_test)\n",
    "\n",
    "accuracy_score_rf = metrics.accuracy_score(prediction,y_test)\n",
    "\n",
    "#evaluation\n",
    "print \"Accuracy :\",accuracy_score_rf \n",
    "print metrics.confusion_matrix(prediction,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree\n",
    "The decision tree scored with default settings only 0.877604166667. Tuning parameters will not get this classifier to the accurracy scores of other classifiers. When looking at the confusion matrix the 0 is the only number that does not get confused for other numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.877604166667\n",
      "[[42  0  1  0  0  0  0  0  0  0]\n",
      " [ 0 32  1  0  0  0  1  0  0  2]\n",
      " [ 0  1 29  1  0  3  1  0  2  0]\n",
      " [ 0  2  0 37  1  0  0  0  0  5]\n",
      " [ 0  0  0  0 28  1  0  0  0  2]\n",
      " [ 0  1  0  0  0 24  1  0  2  4]\n",
      " [ 0  1  0  0  0  0 29  0  0  0]\n",
      " [ 0  1  1  0  0  0  0 48  2  1]\n",
      " [ 0  1  0  0  0  0  0  1 29  0]\n",
      " [ 0  0  2  2  0  1  0  2  0 39]]\n"
     ]
    }
   ],
   "source": [
    "clfDT = DecisionTreeClassifier()\n",
    "clfDT.fit(X_train, y_train) \n",
    "prediction=clfDT.predict(X_test)\n",
    "\n",
    "accuracy_score_dt = metrics.accuracy_score(prediction,y_test)\n",
    "\n",
    "#evaluation\n",
    "print \"Accuracy:\",accuracy_score_dt\n",
    "print metrics.confusion_matrix(prediction,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN with default parameters: 0.96875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.958333333333\n",
      "[[34  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 36  0  0  1  0  1  0  3  3]\n",
      " [ 0  0 36  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 38  0  1  0  0  1  1]\n",
      " [ 0  0  0  0 39  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 37  0  0  0  0]\n",
      " [ 0  0  0  0  1  0 40  0  0  0]\n",
      " [ 0  0  0  0  1  0  0 36  0  3]\n",
      " [ 0  0  0  0  0  0  0  0 32  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 40]]\n"
     ]
    }
   ],
   "source": [
    "clfKNN = KNeighborsClassifier()\n",
    "# Train the classifier with the training data and training labels\n",
    "clfKNN.fit(X_train, y_train)\n",
    "prediction=clfKNN.predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print \"Accuracy: \", metrics.accuracy_score(prediction,y_test)\n",
    "print metrics.confusion_matrix(prediction,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the KNN we first let it run with default settings. The accurracy score was already 0.96875. So we decided to try to make this higher with tuning the hyper parameters. We decided to run a GridSearch to see what parameters would be the best to use.\n",
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the instance\n",
    "clfKNN = KNeighborsClassifier(n_jobs=-1)\n",
    "#Hyper Parameters Set\n",
    "params = {'n_neighbors':[1,2,3,4,5,6,7,8,9,10],\n",
    "          'leaf_size':[1,2,3,5],\n",
    "          'weights':['uniform', 'distance'],\n",
    "          'algorithm':['auto', 'ball_tree','kd_tree','brute'],\n",
    "         'n_jobs': [-1]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "clfKNN1 = GridSearchCV(clfKNN, param_grid=params, n_jobs=-1, verbose=1)\n",
    "#Learning\n",
    "clfKNN1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best params for KNN with grid search\n",
    "algorithm': 'auto'\n",
    "<br>\n",
    "leaf_size': 1\n",
    "<br>\n",
    "n_jobs': -1\n",
    "<br>\n",
    "n_neighbors': 6\n",
    "<br>\n",
    "weights': 'distance'\n",
    "<br>\n",
    "<br>\n",
    "Running the classifier with the params from the gridsearch did improve the accurracy score. It is now 0.966145833333. When looking at the confusion matrix numbers 4, 5, 6, 8 and 9 get confused. The 0, 1, 2 ,3 and 7 don't get confused for other numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.966145833333\n",
      "[[34  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 36  0  0  1  0  1  0  3  2]\n",
      " [ 0  0 36  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 38  0  1  0  0  1  0]\n",
      " [ 0  0  0  0 39  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 37  0  0  0  0]\n",
      " [ 0  0  0  0  1  0 40  0  0  0]\n",
      " [ 0  0  0  0  1  0  0 36  0  2]\n",
      " [ 0  0  0  0  0  0  0  0 32  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 43]]\n"
     ]
    }
   ],
   "source": [
    "clfKNN = KNeighborsClassifier(n_neighbors = 6, n_jobs = -1, weights= 'distance', leaf_size= 1, algorithm= 'auto')\n",
    "clfKNN.fit(X_train, y_train)\n",
    "prediction=clfKNN.predict(X_test)\n",
    "\n",
    "accuracy_score_knn = metrics.accuracy_score(prediction,y_test)\n",
    "\n",
    "#evaluation\n",
    "print \"Accuracy:\",accuracy_score_knn \n",
    "print metrics.confusion_matrix(prediction,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD \n",
    "The accurracy score when running SGD with default settings is 0.958333333333. When looking at the confusion matrix number 0, 1, 5, 6, 8, and 9 get confused. The 8 and 9 get confused the most often.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.958333333333\n",
      "[[33  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 36  0  0  2  0  1  0  2  2]\n",
      " [ 0  0 35  0  0  1  0  0  1  0]\n",
      " [ 0  0  1 38  0  1  0  0  1  0]\n",
      " [ 0  0  0  0 39  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 36  0  0  0  0]\n",
      " [ 0  0  0  0  1  0 40  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 36  0  1]\n",
      " [ 1  0  0  0  0  0  0  0 32  1]\n",
      " [ 0  0  0  0  0  0  0  0  0 43]]\n"
     ]
    }
   ],
   "source": [
    "clfSGD = linear_model.SGDClassifier(loss = 'log', n_iter =1000)\n",
    "clfSGD.fit(X_train, y_train)\n",
    "prediction=clfSGD.predict(X_test)\n",
    "\n",
    "accuracy_score_sgd = metrics.accuracy_score(prediction,y_test)\n",
    "\n",
    "#evaluation\n",
    "print \"Accuracy:\",accuracy_score_sgd \n",
    "print metrics.confusion_matrix(prediction,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "When running LG with default the score is 0.9583. We decided to tune the parameters with a pipeline and GridSearch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.958333333333\n",
      "[[41  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 38  0  0  0  0  1  0  0  1]\n",
      " [ 0  0 34  0  0  2  0  0  0  1]\n",
      " [ 0  0  0 40  0  0  0  0  0  1]\n",
      " [ 0  0  0  0 28  0  0  0  0  2]\n",
      " [ 1  0  0  0  0 25  1  0  1  0]\n",
      " [ 0  0  0  0  0  0 30  0  0  0]\n",
      " [ 0  1  0  0  0  0  0 51  0  0]\n",
      " [ 0  0  0  0  1  0  0  0 34  1]\n",
      " [ 0  0  0  0  0  2  0  0  0 47]]\n"
     ]
    }
   ],
   "source": [
    "clfLG = LogisticRegression()\n",
    "clfLG.fit(X_train, y_train)\n",
    "prediction=clfLG.predict(X_test)\n",
    "accuracy_score_lg = metrics.accuracy_score(prediction,y_test)\n",
    "\n",
    "#evaluation\n",
    "print \"Accuracy:\",accuracy_score_lg \n",
    "print metrics.confusion_matrix(prediction,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('logreg',LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\"))])\n",
    "parameters = {'logreg__C':np.arange(0.01,100,10)}\n",
    "clfLG = GridSearchCV(pipeline,parameters,cv=5, n_jobs=-1, verbose =2)\n",
    "clfLG.fit(X_train, y_train)\n",
    "print(\"Best parameters:\\n\",clfLG.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Best params gridsearch\n",
    "logreg__C: 10.01\n",
    "<br>\n",
    "<br>\n",
    "When testing the LG classifier with this param the results went down to about 0.966145833333. So the best option for LG is to use these settings settings. When looking at the matrix the 3 is the only number to not get confused for others. The 5 and 9 get confused the most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.966145833333\n",
      "[[41  0  1  0  0  0  1  0  0  0]\n",
      " [ 0 38  0  0  0  0  1  0  0  0]\n",
      " [ 0  0 33  0  0  2  0  1  0  0]\n",
      " [ 0  0  0 40  0  0  0  0  0  1]\n",
      " [ 0  1  0  0 29  0  0  0  0  2]\n",
      " [ 1  0  0  0  0 26  0  0  1  0]\n",
      " [ 0  0  0  0  0  0 30  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 50  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 34  0]\n",
      " [ 0  0  0  0  0  1  0  0  0 50]]\n"
     ]
    }
   ],
   "source": [
    "clfLG = LogisticRegression(random_state=0, solver='lbfgs',  multi_class='multinomial', C =10.01 )\n",
    "clfLG.fit(X_train, y_train)\n",
    "prediction=clfLG.predict(X_test)\n",
    "#evaluation(Accuracy)\n",
    "print \"Accuracy:\",metrics.accuracy_score(prediction,y_test)\n",
    "print metrics.confusion_matrix(prediction,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default SVM with Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.971354166667\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "prediction=clf.predict(X_test)\n",
    "\n",
    "print \"Accuracy:\",metrics.accuracy_score(prediction,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with Nu-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.958333333333\n"
     ]
    }
   ],
   "source": [
    "clf = NuSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "prediction=clf.predict(X_test)\n",
    "\n",
    "print \"Accuracy:\",metrics.accuracy_score(prediction,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with Linear Support Vector Classifiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.966145833333\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "prediction=clf.predict(X_test)\n",
    "\n",
    "print \"Accuracy:\",metrics.accuracy_score(prediction,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these three types of support vector machines, the __SVC__ scores the highest. We will use this one and tweak the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with Gridsearch\n",
    "\n",
    "Using grid search we can find which parameters are the best to get the highest score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel' : ('linear', 'rbf'), 'C': np.arange(1, 2, 0.1), \n",
    "              'gamma': np.arange(.1, .5, 0.05)}\n",
    "\n",
    "clf = GridSearchCV(svm.SVC(), parameters, verbose=1, n_jobs=-1, cv = 5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_test, y_test)\n",
    "\n",
    "print clf.best_params_\n",
    "print clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best params for SVM with grid search\n",
    "kernel': 'rbf'\n",
    "<br>\n",
    "C': 1.1\n",
    "<br>\n",
    "gamma': 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.979166666667\n",
      "[[36  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 39  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 39  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 41  0  1  0  1  0  0]\n",
      " [ 0  0  0  0 33  0  0  1  0  0]\n",
      " [ 0  0  0  0  0 37  0  0  0  1]\n",
      " [ 0  1  0  0  0  1 35  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 40  0  0]\n",
      " [ 1  0  0  0  0  0  0  0 31  0]\n",
      " [ 0  1  0  0  0  0  0  0  0 45]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99        36\n",
      "          1       0.95      1.00      0.97        39\n",
      "          2       1.00      1.00      1.00        39\n",
      "          3       1.00      0.95      0.98        43\n",
      "          4       1.00      0.97      0.99        34\n",
      "          5       0.95      0.97      0.96        38\n",
      "          6       1.00      0.95      0.97        37\n",
      "          7       0.95      1.00      0.98        40\n",
      "          8       1.00      0.97      0.98        32\n",
      "          9       0.98      0.98      0.98        46\n",
      "\n",
      "avg / total       0.98      0.98      0.98       384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel =\"rbf\", C=1.1, gamma=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print 'accuracy: ', metrics.accuracy_score(predictions, y_test)\n",
    "print metrics.confusion_matrix (y_test, predictions)\n",
    "print metrics.classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM scores fairly high with a score of 0.979166666667. The numbers 0,1 5, 7, and 9 get confused for other numbers. The most often a number get's confused is two times. This is the best compared to other classifiers. The precision shows that all numbers get predicted right above 95% of the time. \n",
    "We run the SVM with the parameters found above a 100 times with different train and test set. Then we get the mean of all the scores. This way we don't have to worry about outliers. This scores __0.980729166667__ on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy:  0.980729166667\n"
     ]
    }
   ],
   "source": [
    "scores = np.zeros((100))\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.20)\n",
    "    clf = svm.SVC(kernel =\"rbf\", C=1.1, gamma=0.2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores[i] = clf.score(X_test, y_test)\n",
    "\n",
    "print 'mean accuracy: ', scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with PCA  : 0.978072916667\n",
    "The SVM scores a little better without PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy:  0.978072916667\n"
     ]
    }
   ],
   "source": [
    "scores = np.zeros((100))\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.20)\n",
    "    pca = PCA(0.95)\n",
    "    pca.fit(X_train)\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    clf = svm.SVC(kernel =\"rbf\", C=1.1, gamma=0.2)\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "    scores[i] = clf.score(X_test_pca, y_test)\n",
    "\n",
    "print 'mean accuracy: ', scores.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
